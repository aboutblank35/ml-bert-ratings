{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:34:39.899391Z",
     "start_time": "2025-01-18T14:34:39.537915Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd",
   "id": "69932f050b17e644",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:37:26.126318Z",
     "start_time": "2025-01-18T14:37:25.885866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('./data/merged_reviews_metadata_renamed.csv', sep=';', nrows=10000)\n",
    "print(df.shape)\n",
    "df['combined_text'] = df['review_title'] + \" \" + df['review_text']\n",
    "df = df[['review_rating', 'combined_text']]"
   ],
   "id": "8f6181563f978bfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:37:28.770754Z",
     "start_time": "2025-01-18T14:37:28.165995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df.dropna(subset=['combined_text'])\n",
    "test_df = test_df.dropna(subset=['combined_text'])\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ],
   "id": "c7c9c0b0532c47ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7979, 2)\n",
      "(1993, 2)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:37:45.462066Z",
     "start_time": "2025-01-18T14:37:31.776379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(list(train_df['combined_text']),\n",
    "                   padding=True, truncation=True, max_length=512,\n",
    "                   return_tensors=\"pt\")"
   ],
   "id": "a1cbfee7856bd1f4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/usr/local/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/gh/q7935tz54zs8ng8v8hgk1nj40000gn/T/ipykernel_33688/269341999.py\", line 1, in <module>\n",
      "    from transformers import BertTokenizer\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 27, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/transformers/utils/chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/can/Projects/Python/ml-bert-ratings/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:41:44.914611Z",
     "start_time": "2025-01-18T14:40:15.179473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n"
   ],
   "id": "ce6ebe16c51abf7c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:49:16.893322Z",
     "start_time": "2025-01-18T14:49:16.887357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = list(labels)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]  # Annahme: labels zwischen 1 und 5\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        # Falls Labels als 1-5 vorliegen, auf 0-4 anpassen:\n",
    "        item['labels'] = torch.tensor(label - 1)\n",
    "        return item\n"
   ],
   "id": "5dde36377637e36e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T14:49:37.252901Z",
     "start_time": "2025-01-18T14:49:37.242500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ReviewDataset(train_df['combined_text'], train_df['review_rating'], tokenizer)\n",
    "test_dataset = ReviewDataset(test_df['combined_text'], test_df['review_rating'], tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n"
   ],
   "id": "d720f0cfd7854204",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:15:58.738521Z",
     "start_time": "2025-01-18T15:00:31.018041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.optim import Adam  # Alternativer Optimizer\n",
    "\n",
    "# Gerät auswählen: MPS (für M1), ansonsten CUDA oder CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Verwende Device:\", device)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Erstelle den Optimizer mit einem optionalen weight_decay\n",
    "optimizer = Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(train_loader)\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{epochs} ---\")\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):\n",
    "        # Verschiebe alle Elemente des Batches auf das ausgewählte Device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)   # Erwartet: input_ids, attention_mask, labels, etc.\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Gebe alle 10 Schritte Infos aus\n",
    "        if i % 10 == 0 or i == num_batches:\n",
    "            avg_loss = running_loss / i\n",
    "            print(f\"Batch {i}/{num_batches} - Aktuelle durchschnittliche Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    avg_epoch_loss = running_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1} abgeschlossen. Durchschnittliche Loss: {avg_epoch_loss:.4f}\")\n"
   ],
   "id": "382df565cef96385",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende Device: mps\n",
      "\n",
      "--- Epoch 1/3 ---\n",
      "Batch 10/499 - Aktuelle durchschnittliche Loss: 1.1826\n",
      "Batch 20/499 - Aktuelle durchschnittliche Loss: 1.1290\n",
      "Batch 30/499 - Aktuelle durchschnittliche Loss: 1.0833\n",
      "Batch 40/499 - Aktuelle durchschnittliche Loss: 1.0289\n",
      "Batch 50/499 - Aktuelle durchschnittliche Loss: 1.0052\n",
      "Batch 60/499 - Aktuelle durchschnittliche Loss: 0.9818\n",
      "Batch 70/499 - Aktuelle durchschnittliche Loss: 0.9605\n",
      "Batch 80/499 - Aktuelle durchschnittliche Loss: 0.9220\n",
      "Batch 90/499 - Aktuelle durchschnittliche Loss: 0.9141\n",
      "Batch 100/499 - Aktuelle durchschnittliche Loss: 0.9071\n",
      "Batch 110/499 - Aktuelle durchschnittliche Loss: 0.8923\n",
      "Batch 120/499 - Aktuelle durchschnittliche Loss: 0.8809\n",
      "Batch 130/499 - Aktuelle durchschnittliche Loss: 0.8763\n",
      "Batch 140/499 - Aktuelle durchschnittliche Loss: 0.8581\n",
      "Batch 150/499 - Aktuelle durchschnittliche Loss: 0.8528\n",
      "Batch 160/499 - Aktuelle durchschnittliche Loss: 0.8442\n",
      "Batch 170/499 - Aktuelle durchschnittliche Loss: 0.8325\n",
      "Batch 180/499 - Aktuelle durchschnittliche Loss: 0.8248\n",
      "Batch 190/499 - Aktuelle durchschnittliche Loss: 0.8146\n",
      "Batch 200/499 - Aktuelle durchschnittliche Loss: 0.8041\n",
      "Batch 210/499 - Aktuelle durchschnittliche Loss: 0.8011\n",
      "Batch 220/499 - Aktuelle durchschnittliche Loss: 0.7963\n",
      "Batch 230/499 - Aktuelle durchschnittliche Loss: 0.7940\n",
      "Batch 240/499 - Aktuelle durchschnittliche Loss: 0.7898\n",
      "Batch 250/499 - Aktuelle durchschnittliche Loss: 0.7936\n",
      "Batch 260/499 - Aktuelle durchschnittliche Loss: 0.7914\n",
      "Batch 270/499 - Aktuelle durchschnittliche Loss: 0.7886\n",
      "Batch 280/499 - Aktuelle durchschnittliche Loss: 0.7827\n",
      "Batch 290/499 - Aktuelle durchschnittliche Loss: 0.7782\n",
      "Batch 300/499 - Aktuelle durchschnittliche Loss: 0.7714\n",
      "Batch 310/499 - Aktuelle durchschnittliche Loss: 0.7706\n",
      "Batch 320/499 - Aktuelle durchschnittliche Loss: 0.7686\n",
      "Batch 330/499 - Aktuelle durchschnittliche Loss: 0.7653\n",
      "Batch 340/499 - Aktuelle durchschnittliche Loss: 0.7617\n",
      "Batch 350/499 - Aktuelle durchschnittliche Loss: 0.7569\n",
      "Batch 360/499 - Aktuelle durchschnittliche Loss: 0.7535\n",
      "Batch 370/499 - Aktuelle durchschnittliche Loss: 0.7536\n",
      "Batch 380/499 - Aktuelle durchschnittliche Loss: 0.7526\n",
      "Batch 390/499 - Aktuelle durchschnittliche Loss: 0.7505\n",
      "Batch 400/499 - Aktuelle durchschnittliche Loss: 0.7439\n",
      "Batch 410/499 - Aktuelle durchschnittliche Loss: 0.7423\n",
      "Batch 420/499 - Aktuelle durchschnittliche Loss: 0.7374\n",
      "Batch 430/499 - Aktuelle durchschnittliche Loss: 0.7350\n",
      "Batch 440/499 - Aktuelle durchschnittliche Loss: 0.7309\n",
      "Batch 450/499 - Aktuelle durchschnittliche Loss: 0.7289\n",
      "Batch 460/499 - Aktuelle durchschnittliche Loss: 0.7262\n",
      "Batch 470/499 - Aktuelle durchschnittliche Loss: 0.7248\n",
      "Batch 480/499 - Aktuelle durchschnittliche Loss: 0.7233\n",
      "Batch 490/499 - Aktuelle durchschnittliche Loss: 0.7215\n",
      "Batch 499/499 - Aktuelle durchschnittliche Loss: 0.7186\n",
      "Epoch 1 abgeschlossen. Durchschnittliche Loss: 0.7186\n",
      "\n",
      "--- Epoch 2/3 ---\n",
      "Batch 10/499 - Aktuelle durchschnittliche Loss: 0.5128\n",
      "Batch 20/499 - Aktuelle durchschnittliche Loss: 0.5662\n",
      "Batch 30/499 - Aktuelle durchschnittliche Loss: 0.5625\n",
      "Batch 40/499 - Aktuelle durchschnittliche Loss: 0.5460\n",
      "Batch 50/499 - Aktuelle durchschnittliche Loss: 0.5464\n",
      "Batch 60/499 - Aktuelle durchschnittliche Loss: 0.5379\n",
      "Batch 70/499 - Aktuelle durchschnittliche Loss: 0.5394\n",
      "Batch 80/499 - Aktuelle durchschnittliche Loss: 0.5355\n",
      "Batch 90/499 - Aktuelle durchschnittliche Loss: 0.5362\n",
      "Batch 100/499 - Aktuelle durchschnittliche Loss: 0.5342\n",
      "Batch 110/499 - Aktuelle durchschnittliche Loss: 0.5393\n",
      "Batch 120/499 - Aktuelle durchschnittliche Loss: 0.5427\n",
      "Batch 130/499 - Aktuelle durchschnittliche Loss: 0.5654\n",
      "Batch 140/499 - Aktuelle durchschnittliche Loss: 0.5700\n",
      "Batch 150/499 - Aktuelle durchschnittliche Loss: 0.5708\n",
      "Batch 160/499 - Aktuelle durchschnittliche Loss: 0.5668\n",
      "Batch 170/499 - Aktuelle durchschnittliche Loss: 0.5609\n",
      "Batch 180/499 - Aktuelle durchschnittliche Loss: 0.5519\n",
      "Batch 190/499 - Aktuelle durchschnittliche Loss: 0.5525\n",
      "Batch 200/499 - Aktuelle durchschnittliche Loss: 0.5541\n",
      "Batch 210/499 - Aktuelle durchschnittliche Loss: 0.5578\n",
      "Batch 220/499 - Aktuelle durchschnittliche Loss: 0.5584\n",
      "Batch 230/499 - Aktuelle durchschnittliche Loss: 0.5596\n",
      "Batch 240/499 - Aktuelle durchschnittliche Loss: 0.5621\n",
      "Batch 250/499 - Aktuelle durchschnittliche Loss: 0.5611\n",
      "Batch 260/499 - Aktuelle durchschnittliche Loss: 0.5666\n",
      "Batch 270/499 - Aktuelle durchschnittliche Loss: 0.5689\n",
      "Batch 280/499 - Aktuelle durchschnittliche Loss: 0.5667\n",
      "Batch 290/499 - Aktuelle durchschnittliche Loss: 0.5658\n",
      "Batch 300/499 - Aktuelle durchschnittliche Loss: 0.5655\n",
      "Batch 310/499 - Aktuelle durchschnittliche Loss: 0.5643\n",
      "Batch 320/499 - Aktuelle durchschnittliche Loss: 0.5631\n",
      "Batch 330/499 - Aktuelle durchschnittliche Loss: 0.5643\n",
      "Batch 340/499 - Aktuelle durchschnittliche Loss: 0.5620\n",
      "Batch 350/499 - Aktuelle durchschnittliche Loss: 0.5577\n",
      "Batch 360/499 - Aktuelle durchschnittliche Loss: 0.5528\n",
      "Batch 370/499 - Aktuelle durchschnittliche Loss: 0.5513\n",
      "Batch 380/499 - Aktuelle durchschnittliche Loss: 0.5492\n",
      "Batch 390/499 - Aktuelle durchschnittliche Loss: 0.5466\n",
      "Batch 400/499 - Aktuelle durchschnittliche Loss: 0.5470\n",
      "Batch 410/499 - Aktuelle durchschnittliche Loss: 0.5487\n",
      "Batch 420/499 - Aktuelle durchschnittliche Loss: 0.5507\n",
      "Batch 430/499 - Aktuelle durchschnittliche Loss: 0.5504\n",
      "Batch 440/499 - Aktuelle durchschnittliche Loss: 0.5526\n",
      "Batch 450/499 - Aktuelle durchschnittliche Loss: 0.5527\n",
      "Batch 460/499 - Aktuelle durchschnittliche Loss: 0.5533\n",
      "Batch 470/499 - Aktuelle durchschnittliche Loss: 0.5531\n",
      "Batch 480/499 - Aktuelle durchschnittliche Loss: 0.5503\n",
      "Batch 490/499 - Aktuelle durchschnittliche Loss: 0.5505\n",
      "Batch 499/499 - Aktuelle durchschnittliche Loss: 0.5523\n",
      "Epoch 2 abgeschlossen. Durchschnittliche Loss: 0.5523\n",
      "\n",
      "--- Epoch 3/3 ---\n",
      "Batch 10/499 - Aktuelle durchschnittliche Loss: 0.5229\n",
      "Batch 20/499 - Aktuelle durchschnittliche Loss: 0.4806\n",
      "Batch 30/499 - Aktuelle durchschnittliche Loss: 0.5064\n",
      "Batch 40/499 - Aktuelle durchschnittliche Loss: 0.5113\n",
      "Batch 50/499 - Aktuelle durchschnittliche Loss: 0.5068\n",
      "Batch 60/499 - Aktuelle durchschnittliche Loss: 0.5091\n",
      "Batch 70/499 - Aktuelle durchschnittliche Loss: 0.5133\n",
      "Batch 80/499 - Aktuelle durchschnittliche Loss: 0.5237\n",
      "Batch 90/499 - Aktuelle durchschnittliche Loss: 0.5359\n",
      "Batch 100/499 - Aktuelle durchschnittliche Loss: 0.5408\n",
      "Batch 110/499 - Aktuelle durchschnittliche Loss: 0.5386\n",
      "Batch 120/499 - Aktuelle durchschnittliche Loss: 0.5339\n",
      "Batch 130/499 - Aktuelle durchschnittliche Loss: 0.5306\n",
      "Batch 140/499 - Aktuelle durchschnittliche Loss: 0.5264\n",
      "Batch 150/499 - Aktuelle durchschnittliche Loss: 0.5231\n",
      "Batch 160/499 - Aktuelle durchschnittliche Loss: 0.5188\n",
      "Batch 170/499 - Aktuelle durchschnittliche Loss: 0.5215\n",
      "Batch 180/499 - Aktuelle durchschnittliche Loss: 0.5205\n",
      "Batch 190/499 - Aktuelle durchschnittliche Loss: 0.5230\n",
      "Batch 200/499 - Aktuelle durchschnittliche Loss: 0.5220\n",
      "Batch 210/499 - Aktuelle durchschnittliche Loss: 0.5246\n",
      "Batch 220/499 - Aktuelle durchschnittliche Loss: 0.5272\n",
      "Batch 230/499 - Aktuelle durchschnittliche Loss: 0.5281\n",
      "Batch 240/499 - Aktuelle durchschnittliche Loss: 0.5343\n",
      "Batch 250/499 - Aktuelle durchschnittliche Loss: 0.5337\n",
      "Batch 260/499 - Aktuelle durchschnittliche Loss: 0.5318\n",
      "Batch 270/499 - Aktuelle durchschnittliche Loss: 0.5317\n",
      "Batch 280/499 - Aktuelle durchschnittliche Loss: 0.5314\n",
      "Batch 290/499 - Aktuelle durchschnittliche Loss: 0.5352\n",
      "Batch 300/499 - Aktuelle durchschnittliche Loss: 0.5363\n",
      "Batch 310/499 - Aktuelle durchschnittliche Loss: 0.5343\n",
      "Batch 320/499 - Aktuelle durchschnittliche Loss: 0.5323\n",
      "Batch 330/499 - Aktuelle durchschnittliche Loss: 0.5339\n",
      "Batch 340/499 - Aktuelle durchschnittliche Loss: 0.5319\n",
      "Batch 350/499 - Aktuelle durchschnittliche Loss: 0.5293\n",
      "Batch 360/499 - Aktuelle durchschnittliche Loss: 0.5269\n",
      "Batch 370/499 - Aktuelle durchschnittliche Loss: 0.5242\n",
      "Batch 380/499 - Aktuelle durchschnittliche Loss: 0.5257\n",
      "Batch 390/499 - Aktuelle durchschnittliche Loss: 0.5276\n",
      "Batch 400/499 - Aktuelle durchschnittliche Loss: 0.5310\n",
      "Batch 410/499 - Aktuelle durchschnittliche Loss: 0.5325\n",
      "Batch 420/499 - Aktuelle durchschnittliche Loss: 0.5300\n",
      "Batch 430/499 - Aktuelle durchschnittliche Loss: 0.5303\n",
      "Batch 440/499 - Aktuelle durchschnittliche Loss: 0.5287\n",
      "Batch 450/499 - Aktuelle durchschnittliche Loss: 0.5305\n",
      "Batch 460/499 - Aktuelle durchschnittliche Loss: 0.5273\n",
      "Batch 470/499 - Aktuelle durchschnittliche Loss: 0.5302\n",
      "Batch 480/499 - Aktuelle durchschnittliche Loss: 0.5299\n",
      "Batch 490/499 - Aktuelle durchschnittliche Loss: 0.5308\n",
      "Batch 499/499 - Aktuelle durchschnittliche Loss: 0.5301\n",
      "Epoch 3 abgeschlossen. Durchschnittliche Loss: 0.5301\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:18:06.825693Z",
     "start_time": "2025-01-18T16:18:05.160334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Speichern beider Varianten\n",
    "torch.save(model.state_dict(), \"./data/models/model_weights.pth\")\n",
    "torch.save(model, \"./data/models/model_full.pth\")\n",
    "print(\"Modell und Gewichte wurden in './data/models/' gespeichert.\")"
   ],
   "id": "749abd200cf25335",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell und Gewichte wurden in './data/models/' gespeichert.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:26:53.809608Z",
     "start_time": "2025-01-18T16:26:12.237425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model.eval()  # Modell in den Evaluationsmodus schalten\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "batch_count = 0\n",
    "\n",
    "print(\"Starte die Evaluation...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch_count += 1\n",
    "        print(f\"\\n--- Batch {batch_count}/{len(test_loader)} ---\")\n",
    "\n",
    "        # Batch auf das richtige Device verschieben (MPS, CUDA oder CPU)\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        print(f\"Predicted Labels: {preds.cpu().tolist()}\")\n",
    "        print(f\"Ground Truth Labels: {batch['labels'].cpu().tolist()}\")\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch['labels'].cpu().tolist())\n",
    "\n",
    "print(\"\\nEvaluation abgeschlossen. Bereite finale Ergebnisse vor...\")\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "report = classification_report(all_labels, all_preds, target_names=[f\"Klasse {i}\" for i in range(5)])\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)"
   ],
   "id": "ae90060260009cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte die Evaluation...\n",
      "\n",
      "--- Batch 1/125 ---\n",
      "Batch Loss: 0.7375\n",
      "Predicted Labels: [3, 3, 4, 4, 4, 1, 0, 3, 4, 4, 4, 1, 4, 4, 2, 4]\n",
      "Ground Truth Labels: [3, 4, 3, 4, 4, 1, 0, 3, 4, 4, 3, 1, 4, 4, 3, 4]\n",
      "\n",
      "--- Batch 2/125 ---\n",
      "Batch Loss: 0.4189\n",
      "Predicted Labels: [4, 4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 0, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 1, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 1, 4]\n",
      "\n",
      "--- Batch 3/125 ---\n",
      "Batch Loss: 1.0483\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 4, 1, 3, 1, 4]\n",
      "Ground Truth Labels: [4, 3, 0, 2, 3, 4, 4, 4, 4, 2, 4, 3, 3, 4, 0, 4]\n",
      "\n",
      "--- Batch 4/125 ---\n",
      "Batch Loss: 0.4805\n",
      "Predicted Labels: [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 2]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 3, 3, 2]\n",
      "\n",
      "--- Batch 5/125 ---\n",
      "Batch Loss: 0.6580\n",
      "Predicted Labels: [0, 4, 0, 4, 4, 1, 2, 0, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "Ground Truth Labels: [2, 4, 0, 4, 3, 2, 2, 1, 3, 4, 4, 4, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 6/125 ---\n",
      "Batch Loss: 0.4023\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 3, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 7/125 ---\n",
      "Batch Loss: 0.4933\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 4, 4, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4, 4, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 8/125 ---\n",
      "Batch Loss: 0.8725\n",
      "Predicted Labels: [4, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 1, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 2, 2, 4, 4, 4, 4, 4, 1, 2, 4, 2, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 9/125 ---\n",
      "Batch Loss: 0.5859\n",
      "Predicted Labels: [3, 1, 4, 4, 4, 3, 3, 4, 0, 2, 4, 4, 0, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 1, 4, 4, 4, 2, 3, 4, 1, 2, 4, 4, 1, 0, 3, 4]\n",
      "\n",
      "--- Batch 10/125 ---\n",
      "Batch Loss: 0.9219\n",
      "Predicted Labels: [4, 0, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4]\n",
      "Ground Truth Labels: [4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 1, 3]\n",
      "\n",
      "--- Batch 11/125 ---\n",
      "Batch Loss: 0.3184\n",
      "Predicted Labels: [3, 4, 0, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2]\n",
      "Ground Truth Labels: [3, 4, 0, 0, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "--- Batch 12/125 ---\n",
      "Batch Loss: 0.7471\n",
      "Predicted Labels: [4, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 2, 3, 4, 4]\n",
      "Ground Truth Labels: [3, 3, 3, 4, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 13/125 ---\n",
      "Batch Loss: 0.8269\n",
      "Predicted Labels: [0, 4, 4, 4, 0, 0, 4, 4, 4, 0, 4, 4, 0, 4, 4, 0]\n",
      "Ground Truth Labels: [3, 4, 4, 4, 0, 4, 4, 4, 4, 0, 4, 4, 1, 4, 3, 0]\n",
      "\n",
      "--- Batch 14/125 ---\n",
      "Batch Loss: 0.7233\n",
      "Predicted Labels: [4, 4, 4, 3, 4, 4, 4, 0, 4, 4, 0, 4, 0, 4, 1, 4]\n",
      "Ground Truth Labels: [3, 4, 4, 2, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 1, 4]\n",
      "\n",
      "--- Batch 15/125 ---\n",
      "Batch Loss: 0.7428\n",
      "Predicted Labels: [4, 4, 4, 0, 4, 4, 3, 4, 3, 2, 4, 2, 4, 4, 4, 2]\n",
      "Ground Truth Labels: [3, 4, 4, 0, 4, 4, 2, 4, 3, 1, 4, 4, 3, 4, 4, 3]\n",
      "\n",
      "--- Batch 16/125 ---\n",
      "Batch Loss: 0.5127\n",
      "Predicted Labels: [4, 3, 4, 4, 4, 4, 0, 4, 2, 4, 1, 4, 4, 0, 4, 0]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 4, 4, 1, 4, 1, 4, 1, 4, 4, 0, 4, 0]\n",
      "\n",
      "--- Batch 17/125 ---\n",
      "Batch Loss: 0.8068\n",
      "Predicted Labels: [4, 4, 1, 0, 4, 3, 4, 4, 2, 3, 4, 4, 4, 4, 2, 4]\n",
      "Ground Truth Labels: [4, 3, 1, 0, 4, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 18/125 ---\n",
      "Batch Loss: 0.7331\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 3, 2, 4, 3, 2, 4]\n",
      "Ground Truth Labels: [0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 1, 2, 4, 3, 2, 4]\n",
      "\n",
      "--- Batch 19/125 ---\n",
      "Batch Loss: 0.3721\n",
      "Predicted Labels: [4, 3, 4, 4, 4, 3, 2, 1, 4, 4, 4, 4, 0, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 3, 2, 1, 4, 4, 4, 4, 0, 0, 4, 4]\n",
      "\n",
      "--- Batch 20/125 ---\n",
      "Batch Loss: 0.5041\n",
      "Predicted Labels: [0, 4, 4, 1, 2, 4, 3, 4, 4, 0, 4, 4, 4, 4, 0, 4]\n",
      "Ground Truth Labels: [1, 4, 4, 1, 3, 4, 4, 4, 4, 1, 4, 4, 4, 4, 0, 4]\n",
      "\n",
      "--- Batch 21/125 ---\n",
      "Batch Loss: 0.4491\n",
      "Predicted Labels: [0, 1, 4, 4, 4, 4, 4, 0, 1, 4, 0, 4, 0, 4, 4, 0]\n",
      "Ground Truth Labels: [0, 3, 4, 4, 4, 4, 4, 0, 1, 4, 0, 4, 1, 4, 4, 0]\n",
      "\n",
      "--- Batch 22/125 ---\n",
      "Batch Loss: 0.3857\n",
      "Predicted Labels: [4, 0, 4, 4, 4, 2, 3, 3, 4, 4, 0, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 0, 3, 4, 4, 2, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 23/125 ---\n",
      "Batch Loss: 0.4707\n",
      "Predicted Labels: [4, 3, 4, 0, 4, 4, 0, 2, 4, 3, 0, 4, 4, 2, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 0, 4, 4, 1, 4, 4, 3, 0, 4, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 24/125 ---\n",
      "Batch Loss: 0.7811\n",
      "Predicted Labels: [4, 2, 4, 4, 4, 2, 3, 1, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 3, 2, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 25/125 ---\n",
      "Batch Loss: 0.6787\n",
      "Predicted Labels: [4, 3, 4, 0, 4, 4, 0, 4, 4, 4, 4, 2, 0, 2, 4, 4]\n",
      "Ground Truth Labels: [4, 2, 4, 0, 4, 4, 3, 4, 4, 4, 3, 2, 1, 2, 4, 4]\n",
      "\n",
      "--- Batch 26/125 ---\n",
      "Batch Loss: 0.5595\n",
      "Predicted Labels: [4, 4, 4, 0, 4, 4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 0, 4, 4, 4, 3, 4, 4, 4, 3, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 27/125 ---\n",
      "Batch Loss: 0.7789\n",
      "Predicted Labels: [2, 4, 4, 4, 3, 1, 4, 3, 4, 4, 3, 4, 0, 4, 0, 4]\n",
      "Ground Truth Labels: [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 0, 3]\n",
      "\n",
      "--- Batch 28/125 ---\n",
      "Batch Loss: 0.7171\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 0, 4, 4, 3, 4, 4, 4, 4, 0, 3]\n",
      "Ground Truth Labels: [4, 4, 0, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "--- Batch 29/125 ---\n",
      "Batch Loss: 0.6993\n",
      "Predicted Labels: [4, 0, 4, 0, 4, 2, 4, 4, 4, 0, 4, 4, 4, 3, 4, 0]\n",
      "Ground Truth Labels: [3, 0, 4, 2, 4, 2, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 30/125 ---\n",
      "Batch Loss: 0.6699\n",
      "Predicted Labels: [4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 0, 4, 3, 4, 4, 4, 4, 4, 3, 2, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 31/125 ---\n",
      "Batch Loss: 0.9752\n",
      "Predicted Labels: [0, 1, 0, 4, 1, 0, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4]\n",
      "Ground Truth Labels: [2, 2, 4, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 32/125 ---\n",
      "Batch Loss: 1.0666\n",
      "Predicted Labels: [4, 2, 0, 4, 0, 3, 4, 1, 4, 4, 4, 4, 4, 2, 2, 4]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 0, 1, 4, 3, 4, 4, 4, 4, 4, 3, 3, 4]\n",
      "\n",
      "--- Batch 33/125 ---\n",
      "Batch Loss: 0.7519\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 1, 4, 4, 0, 4, 4, 4, 4, 4, 3, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 3, 1, 2, 4, 0, 4, 4, 4, 4, 4, 3, 4]\n",
      "\n",
      "--- Batch 34/125 ---\n",
      "Batch Loss: 0.8005\n",
      "Predicted Labels: [4, 4, 0, 4, 2, 4, 0, 4, 4, 4, 4, 0, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 0, 4, 0, 4, 1, 4, 4, 4, 4, 0, 3, 4, 4, 3]\n",
      "\n",
      "--- Batch 35/125 ---\n",
      "Batch Loss: 0.6234\n",
      "Predicted Labels: [4, 4, 4, 4, 0, 4, 2, 0, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 0, 4, 2, 1, 2, 4, 3, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 36/125 ---\n",
      "Batch Loss: 0.6333\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 1, 1, 3]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 2, 3, 2, 4]\n",
      "\n",
      "--- Batch 37/125 ---\n",
      "Batch Loss: 0.5199\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 3, 4, 3, 2, 2, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 38/125 ---\n",
      "Batch Loss: 0.4824\n",
      "Predicted Labels: [2, 4, 4, 4, 4, 2, 4, 0, 4, 0, 4, 4, 3, 4, 4, 2]\n",
      "Ground Truth Labels: [1, 4, 4, 3, 4, 4, 4, 0, 4, 0, 4, 4, 3, 4, 4, 2]\n",
      "\n",
      "--- Batch 39/125 ---\n",
      "Batch Loss: 0.6252\n",
      "Predicted Labels: [4, 4, 4, 4, 0, 4, 4, 0, 2, 0, 4, 3, 0, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 1, 4, 4, 0, 3, 0, 4, 2, 0, 2, 4, 4]\n",
      "\n",
      "--- Batch 40/125 ---\n",
      "Batch Loss: 0.5570\n",
      "Predicted Labels: [4, 1, 1, 4, 0, 4, 4, 4, 0, 4, 4, 4, 2, 2, 2, 4]\n",
      "Ground Truth Labels: [4, 2, 1, 4, 0, 4, 4, 3, 1, 4, 4, 4, 3, 4, 2, 4]\n",
      "\n",
      "--- Batch 41/125 ---\n",
      "Batch Loss: 0.5809\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 0, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 4, 0, 3, 4, 0, 4, 4, 4, 4, 3, 3, 4]\n",
      "\n",
      "--- Batch 42/125 ---\n",
      "Batch Loss: 0.6284\n",
      "Predicted Labels: [0, 4, 2, 3, 4, 4, 4, 4, 0, 1, 2, 3, 4, 4, 2, 4]\n",
      "Ground Truth Labels: [0, 4, 2, 3, 3, 4, 4, 4, 0, 2, 2, 3, 4, 4, 1, 4]\n",
      "\n",
      "--- Batch 43/125 ---\n",
      "Batch Loss: 0.9012\n",
      "Predicted Labels: [4, 0, 2, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4]\n",
      "Ground Truth Labels: [4, 0, 0, 0, 3, 2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4]\n",
      "\n",
      "--- Batch 44/125 ---\n",
      "Batch Loss: 0.4859\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 0, 4, 0, 0, 4, 4, 1, 4, 3, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 1, 4, 1, 1, 4, 4, 0, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 45/125 ---\n",
      "Batch Loss: 0.1925\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 46/125 ---\n",
      "Batch Loss: 0.6595\n",
      "Predicted Labels: [4, 4, 1, 3, 4, 3, 4, 4, 4, 4, 2, 0, 3, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 1, 3, 4, 4, 3, 4, 4, 4, 4, 0, 3, 3, 4, 4]\n",
      "\n",
      "--- Batch 47/125 ---\n",
      "Batch Loss: 0.2462\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 3, 4, 1, 4, 4, 4, 0, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 0, 4, 4, 0]\n",
      "\n",
      "--- Batch 48/125 ---\n",
      "Batch Loss: 0.8008\n",
      "Predicted Labels: [0, 2, 4, 4, 2, 4, 4, 4, 0, 4, 4, 0, 4, 4, 0, 4]\n",
      "Ground Truth Labels: [0, 2, 4, 4, 1, 4, 4, 4, 1, 4, 4, 1, 3, 4, 1, 3]\n",
      "\n",
      "--- Batch 49/125 ---\n",
      "Batch Loss: 0.2624\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 0, 1]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 3, 0, 2]\n",
      "\n",
      "--- Batch 50/125 ---\n",
      "Batch Loss: 0.4923\n",
      "Predicted Labels: [4, 0, 4, 4, 4, 0, 0, 4, 2, 0, 4, 0, 4, 2, 4, 4]\n",
      "Ground Truth Labels: [4, 1, 4, 3, 4, 0, 0, 4, 2, 0, 4, 0, 4, 2, 4, 4]\n",
      "\n",
      "--- Batch 51/125 ---\n",
      "Batch Loss: 0.7143\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 4, 4, 3, 4, 4, 0, 2, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 3, 0]\n",
      "\n",
      "--- Batch 52/125 ---\n",
      "Batch Loss: 0.4844\n",
      "Predicted Labels: [4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 0, 3, 4, 4, 3, 4, 4, 4, 1, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 53/125 ---\n",
      "Batch Loss: 0.5009\n",
      "Predicted Labels: [4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 54/125 ---\n",
      "Batch Loss: 0.6750\n",
      "Predicted Labels: [4, 4, 1, 4, 4, 4, 4, 0, 4, 4, 0, 4, 3, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 3, 4, 4, 1, 3, 4, 0, 4, 3, 4, 4, 3]\n",
      "\n",
      "--- Batch 55/125 ---\n",
      "Batch Loss: 0.6546\n",
      "Predicted Labels: [0, 0, 4, 4, 1, 4, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [3, 4, 4, 4, 2, 4, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 56/125 ---\n",
      "Batch Loss: 0.1814\n",
      "Predicted Labels: [4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 57/125 ---\n",
      "Batch Loss: 0.8212\n",
      "Predicted Labels: [4, 4, 4, 0, 4, 4, 2, 4, 4, 2, 4, 4, 3, 0, 3, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 1, 4, 4, 3, 4, 3, 4, 4, 4, 3, 0, 3, 2]\n",
      "\n",
      "--- Batch 58/125 ---\n",
      "Batch Loss: 0.3450\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 0, 4, 1, 4, 4, 4]\n",
      "\n",
      "--- Batch 59/125 ---\n",
      "Batch Loss: 0.7305\n",
      "Predicted Labels: [4, 4, 2, 4, 0, 0, 4, 3, 4, 4, 3, 3, 4, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 0, 0, 3, 4, 4, 2, 4, 3, 4, 4, 4, 0]\n",
      "\n",
      "--- Batch 60/125 ---\n",
      "Batch Loss: 0.5112\n",
      "Predicted Labels: [4, 3, 4, 3, 0, 0, 4, 0, 4, 4, 4, 0, 0, 0, 4, 2]\n",
      "Ground Truth Labels: [4, 2, 4, 3, 0, 0, 4, 0, 4, 4, 4, 2, 0, 0, 4, 3]\n",
      "\n",
      "--- Batch 61/125 ---\n",
      "Batch Loss: 0.5843\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 0, 4, 0, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 0]\n",
      "\n",
      "--- Batch 62/125 ---\n",
      "Batch Loss: 0.2609\n",
      "Predicted Labels: [4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "--- Batch 63/125 ---\n",
      "Batch Loss: 0.3774\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4]\n",
      "\n",
      "--- Batch 64/125 ---\n",
      "Batch Loss: 0.3201\n",
      "Predicted Labels: [4, 4, 4, 0, 4, 4, 4, 1, 4, 4, 2, 4, 3, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 0, 4, 4, 4, 0, 4, 4, 3, 4, 3, 0, 4, 4]\n",
      "\n",
      "--- Batch 65/125 ---\n",
      "Batch Loss: 0.4610\n",
      "Predicted Labels: [4, 1, 4, 4, 0, 2, 4, 2, 4, 4, 4, 3, 4, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 1, 4, 4, 2, 2, 4, 2, 4, 4, 4, 3, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 66/125 ---\n",
      "Batch Loss: 0.6434\n",
      "Predicted Labels: [4, 1, 4, 1, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4]\n",
      "Ground Truth Labels: [4, 0, 4, 2, 3, 4, 4, 4, 4, 4, 4, 3, 3, 4, 1, 4]\n",
      "\n",
      "--- Batch 67/125 ---\n",
      "Batch Loss: 0.5988\n",
      "Predicted Labels: [4, 4, 2, 0, 0, 4, 2, 0, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 1, 0, 0, 4, 4, 0, 3, 4, 4, 3, 4, 1, 4, 4]\n",
      "\n",
      "--- Batch 68/125 ---\n",
      "Batch Loss: 0.9178\n",
      "Predicted Labels: [2, 2, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 3, 4, 4, 4]\n",
      "Ground Truth Labels: [3, 2, 4, 0, 3, 1, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 69/125 ---\n",
      "Batch Loss: 0.5553\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 3, 3, 4]\n",
      "Ground Truth Labels: [3, 4, 3, 4, 4, 4, 3, 4, 4, 4, 3, 4, 4, 3, 3, 4]\n",
      "\n",
      "--- Batch 70/125 ---\n",
      "Batch Loss: 0.2681\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 0]\n",
      "\n",
      "--- Batch 71/125 ---\n",
      "Batch Loss: 0.9455\n",
      "Predicted Labels: [4, 4, 2, 4, 0, 2, 4, 0, 1, 3, 2, 4, 0, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 3, 4, 1, 2, 4, 0, 3, 2, 3, 3, 1, 4, 4, 3]\n",
      "\n",
      "--- Batch 72/125 ---\n",
      "Batch Loss: 0.7602\n",
      "Predicted Labels: [4, 4, 0, 4, 2, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 0, 4, 3, 3, 3, 4, 4, 4, 4, 3, 4, 1, 4, 4]\n",
      "\n",
      "--- Batch 73/125 ---\n",
      "Batch Loss: 0.3300\n",
      "Predicted Labels: [4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 2, 0, 2, 4]\n",
      "Ground Truth Labels: [4, 4, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 1, 2, 4]\n",
      "\n",
      "--- Batch 74/125 ---\n",
      "Batch Loss: 0.9243\n",
      "Predicted Labels: [2, 1, 2, 4, 4, 0, 4, 4, 4, 3, 1, 3, 4, 4, 4, 0]\n",
      "Ground Truth Labels: [4, 1, 4, 4, 3, 0, 4, 4, 4, 3, 3, 3, 4, 4, 4, 0]\n",
      "\n",
      "--- Batch 75/125 ---\n",
      "Batch Loss: 0.2686\n",
      "Predicted Labels: [4, 4, 4, 4, 3, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 3, 4, 0, 4, 1, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 76/125 ---\n",
      "Batch Loss: 0.4881\n",
      "Predicted Labels: [4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 2, 0]\n",
      "Ground Truth Labels: [4, 4, 0, 4, 4, 4, 3, 4, 4, 4, 0, 4, 4, 4, 4, 0]\n",
      "\n",
      "--- Batch 77/125 ---\n",
      "Batch Loss: 0.3912\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "Ground Truth Labels: [3, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 78/125 ---\n",
      "Batch Loss: 0.9315\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 4, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [2, 3, 4, 4, 4, 4, 2, 0, 0, 0, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 79/125 ---\n",
      "Batch Loss: 0.2966\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 80/125 ---\n",
      "Batch Loss: 0.4376\n",
      "Predicted Labels: [4, 4, 4, 4, 1, 4, 4, 4, 4, 4, 3, 1, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 0, 4, 4, 4, 3, 4, 3, 2, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 81/125 ---\n",
      "Batch Loss: 0.8495\n",
      "Predicted Labels: [4, 2, 4, 2, 3, 4, 0, 4, 4, 4, 4, 4, 4, 0, 4, 0]\n",
      "Ground Truth Labels: [4, 2, 2, 2, 3, 4, 2, 4, 4, 4, 4, 4, 4, 0, 4, 4]\n",
      "\n",
      "--- Batch 82/125 ---\n",
      "Batch Loss: 0.6385\n",
      "Predicted Labels: [2, 4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 4, 1, 2]\n",
      "Ground Truth Labels: [2, 3, 4, 4, 4, 3, 4, 4, 3, 0, 4, 4, 3, 4, 1, 2]\n",
      "\n",
      "--- Batch 83/125 ---\n",
      "Batch Loss: 0.5475\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 2, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 2, 4, 4, 0, 1, 4, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 84/125 ---\n",
      "Batch Loss: 0.6722\n",
      "Predicted Labels: [0, 4, 3, 4, 4, 4, 2, 4, 4, 4, 4, 4, 0, 4, 2, 4]\n",
      "Ground Truth Labels: [0, 3, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 1, 4, 2, 4]\n",
      "\n",
      "--- Batch 85/125 ---\n",
      "Batch Loss: 0.2602\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 2, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 4, 4, 4, 2, 0, 4, 3, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 86/125 ---\n",
      "Batch Loss: 0.7985\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 4, 4, 1, 4, 2, 4, 4, 3, 1, 2]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 2, 0, 4]\n",
      "\n",
      "--- Batch 87/125 ---\n",
      "Batch Loss: 0.8531\n",
      "Predicted Labels: [4, 0, 3, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4, 4, 1, 4]\n",
      "Ground Truth Labels: [4, 1, 3, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4]\n",
      "\n",
      "--- Batch 88/125 ---\n",
      "Batch Loss: 0.4105\n",
      "Predicted Labels: [4, 2, 4, 2, 4, 4, 4, 0, 2, 4, 4, 4, 2, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 0, 4, 2, 4, 4, 4, 0, 1, 4, 4, 4, 2, 4, 4, 4]\n",
      "\n",
      "--- Batch 89/125 ---\n",
      "Batch Loss: 0.5052\n",
      "Predicted Labels: [4, 0, 4, 4, 3, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 0, 4, 3, 4, 3, 4, 4, 0, 4, 1, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 90/125 ---\n",
      "Batch Loss: 0.5803\n",
      "Predicted Labels: [3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 2, 0, 4, 3, 3]\n",
      "\n",
      "--- Batch 91/125 ---\n",
      "Batch Loss: 0.4728\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 3, 4, 4, 2, 4, 4, 3, 4, 4, 4, 4, 2, 0]\n",
      "\n",
      "--- Batch 92/125 ---\n",
      "Batch Loss: 0.3497\n",
      "Predicted Labels: [4, 4, 3, 0, 4, 4, 0, 4, 4, 4, 4, 3, 4, 4, 4, 2]\n",
      "Ground Truth Labels: [4, 4, 3, 2, 4, 4, 0, 4, 4, 4, 4, 3, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 93/125 ---\n",
      "Batch Loss: 0.6779\n",
      "Predicted Labels: [4, 0, 4, 4, 4, 0, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 1, 4, 4, 2, 3, 4, 4, 4, 4, 4, 3]\n",
      "\n",
      "--- Batch 94/125 ---\n",
      "Batch Loss: 0.7503\n",
      "Predicted Labels: [4, 0, 4, 4, 4, 2, 1, 4, 4, 3, 4, 4, 3, 0, 4, 4]\n",
      "Ground Truth Labels: [4, 1, 4, 4, 3, 3, 1, 4, 4, 4, 4, 4, 3, 1, 4, 3]\n",
      "\n",
      "--- Batch 95/125 ---\n",
      "Batch Loss: 0.6959\n",
      "Predicted Labels: [0, 4, 4, 4, 4, 4, 3, 4, 0, 2, 4, 4, 4, 3, 0, 0]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 4, 3, 3, 4, 0, 0, 4, 4, 4, 3, 0, 0]\n",
      "\n",
      "--- Batch 96/125 ---\n",
      "Batch Loss: 0.3512\n",
      "Predicted Labels: [4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 97/125 ---\n",
      "Batch Loss: 1.0961\n",
      "Predicted Labels: [4, 4, 4, 0, 4, 0, 0, 4, 4, 4, 4, 4, 4, 4, 3, 4]\n",
      "Ground Truth Labels: [2, 2, 4, 2, 4, 0, 2, 3, 4, 2, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 98/125 ---\n",
      "Batch Loss: 0.9193\n",
      "Predicted Labels: [2, 4, 0, 4, 4, 0, 0, 4, 4, 4, 4, 2, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 1, 4, 2, 1, 2, 4, 4, 4, 4, 2, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 99/125 ---\n",
      "Batch Loss: 0.8113\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 0, 4, 2, 4, 4, 2, 0, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 2, 4, 4, 4, 4, 0, 4, 2, 4, 4, 4, 2, 4, 4, 4]\n",
      "\n",
      "--- Batch 100/125 ---\n",
      "Batch Loss: 0.3331\n",
      "Predicted Labels: [1, 2, 4, 0, 4, 4, 4, 4, 4, 4, 0, 1, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [0, 2, 4, 0, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 101/125 ---\n",
      "Batch Loss: 0.7048\n",
      "Predicted Labels: [4, 4, 3, 4, 3, 4, 4, 0, 4, 4, 1, 4, 4, 1, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 2, 4, 3, 4, 4, 2, 3, 4, 2, 4, 4, 0, 4, 2]\n",
      "\n",
      "--- Batch 102/125 ---\n",
      "Batch Loss: 0.8847\n",
      "Predicted Labels: [2, 0, 4, 4, 4, 4, 4, 2, 4, 4, 3, 2, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [2, 1, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 103/125 ---\n",
      "Batch Loss: 1.0773\n",
      "Predicted Labels: [4, 4, 1, 3, 4, 0, 4, 2, 4, 4, 4, 4, 4, 4, 0, 0]\n",
      "Ground Truth Labels: [4, 4, 3, 2, 4, 2, 4, 1, 3, 4, 4, 3, 4, 4, 0, 1]\n",
      "\n",
      "--- Batch 104/125 ---\n",
      "Batch Loss: 0.4630\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 0, 4, 0, 4, 3, 4, 4, 2, 0, 3, 1]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4, 3, 0, 0, 3, 1]\n",
      "\n",
      "--- Batch 105/125 ---\n",
      "Batch Loss: 0.1254\n",
      "Predicted Labels: [4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 106/125 ---\n",
      "Batch Loss: 0.7031\n",
      "Predicted Labels: [4, 4, 4, 0, 0, 1, 4, 3, 3, 0, 4, 4, 0, 0, 4, 1]\n",
      "Ground Truth Labels: [4, 4, 4, 1, 0, 2, 4, 3, 4, 0, 4, 4, 1, 1, 3, 1]\n",
      "\n",
      "--- Batch 107/125 ---\n",
      "Batch Loss: 0.5146\n",
      "Predicted Labels: [4, 4, 4, 0, 3, 4, 4, 4, 4, 0, 4, 4, 4, 3, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 0, 3, 3, 4, 4, 4, 0, 4, 4, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 108/125 ---\n",
      "Batch Loss: 0.5126\n",
      "Predicted Labels: [4, 2, 0, 4, 3, 4, 4, 4, 1, 4, 4, 4, 3, 4, 0, 0]\n",
      "Ground Truth Labels: [4, 2, 2, 3, 3, 3, 4, 4, 0, 4, 4, 4, 3, 4, 0, 0]\n",
      "\n",
      "--- Batch 109/125 ---\n",
      "Batch Loss: 0.3732\n",
      "Predicted Labels: [0, 4, 4, 4, 0, 4, 2, 4, 0, 4, 4, 4, 2, 3, 4, 4]\n",
      "Ground Truth Labels: [0, 4, 4, 4, 0, 4, 2, 4, 1, 4, 4, 4, 2, 2, 4, 4]\n",
      "\n",
      "--- Batch 110/125 ---\n",
      "Batch Loss: 0.6120\n",
      "Predicted Labels: [4, 4, 4, 1, 4, 0, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 4, 3, 4, 2, 4, 4, 0, 3, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 111/125 ---\n",
      "Batch Loss: 0.5385\n",
      "Predicted Labels: [3, 4, 4, 3, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [2, 4, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 112/125 ---\n",
      "Batch Loss: 0.4515\n",
      "Predicted Labels: [4, 0, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 2]\n",
      "Ground Truth Labels: [4, 0, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 3]\n",
      "\n",
      "--- Batch 113/125 ---\n",
      "Batch Loss: 0.6981\n",
      "Predicted Labels: [4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 3, 4, 4, 0, 4]\n",
      "Ground Truth Labels: [4, 2, 4, 4, 4, 4, 3, 4, 3, 0, 4, 3, 4, 4, 0, 4]\n",
      "\n",
      "--- Batch 114/125 ---\n",
      "Batch Loss: 0.5154\n",
      "Predicted Labels: [4, 0, 4, 0, 2, 4, 4, 4, 3, 4, 4, 4, 2, 4, 4, 3]\n",
      "Ground Truth Labels: [4, 0, 4, 2, 1, 4, 4, 4, 3, 4, 3, 4, 0, 4, 4, 3]\n",
      "\n",
      "--- Batch 115/125 ---\n",
      "Batch Loss: 0.6215\n",
      "Predicted Labels: [4, 4, 0, 0, 4, 0, 4, 4, 2, 4, 4, 0, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 4, 1, 0, 4, 0, 4, 4, 2, 4, 4, 2, 4, 4, 4, 3]\n",
      "\n",
      "--- Batch 116/125 ---\n",
      "Batch Loss: 1.0437\n",
      "Predicted Labels: [4, 4, 4, 4, 3, 4, 4, 4, 4, 2, 3, 2, 0, 4, 1, 4]\n",
      "Ground Truth Labels: [4, 3, 4, 4, 3, 4, 3, 3, 0, 3, 4, 4, 0, 4, 1, 4]\n",
      "\n",
      "--- Batch 117/125 ---\n",
      "Batch Loss: 1.2906\n",
      "Predicted Labels: [0, 1, 4, 4, 4, 0, 4, 4, 3, 4, 3, 4, 4, 2, 1, 4]\n",
      "Ground Truth Labels: [0, 3, 4, 3, 4, 1, 2, 4, 4, 4, 2, 4, 4, 1, 0, 3]\n",
      "\n",
      "--- Batch 118/125 ---\n",
      "Batch Loss: 0.8757\n",
      "Predicted Labels: [4, 4, 4, 4, 1, 2, 2, 4, 1, 4, 4, 4, 4, 4, 0, 0]\n",
      "Ground Truth Labels: [4, 4, 4, 4, 1, 1, 1, 4, 2, 4, 4, 4, 3, 3, 2, 0]\n",
      "\n",
      "--- Batch 119/125 ---\n",
      "Batch Loss: 0.5814\n",
      "Predicted Labels: [4, 0, 2, 0, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 0, 2, 1, 4, 2, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4]\n",
      "\n",
      "--- Batch 120/125 ---\n",
      "Batch Loss: 0.7884\n",
      "Predicted Labels: [4, 4, 0, 4, 1, 0, 4, 0, 4, 4, 4, 0, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [4, 3, 0, 4, 2, 0, 4, 3, 4, 4, 4, 2, 4, 4, 3, 4]\n",
      "\n",
      "--- Batch 121/125 ---\n",
      "Batch Loss: 0.4374\n",
      "Predicted Labels: [4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3]\n",
      "Ground Truth Labels: [4, 4, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4]\n",
      "\n",
      "--- Batch 122/125 ---\n",
      "Batch Loss: 0.9373\n",
      "Predicted Labels: [3, 0, 4, 4, 2, 4, 0, 4, 4, 1, 2, 4, 4, 2, 3, 0]\n",
      "Ground Truth Labels: [4, 1, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 4, 2, 3, 0]\n",
      "\n",
      "--- Batch 123/125 ---\n",
      "Batch Loss: 0.9696\n",
      "Predicted Labels: [4, 4, 0, 0, 3, 4, 4, 4, 0, 4, 4, 4, 0, 4, 0, 3]\n",
      "Ground Truth Labels: [4, 3, 1, 1, 3, 3, 4, 3, 0, 4, 4, 3, 0, 3, 1, 4]\n",
      "\n",
      "--- Batch 124/125 ---\n",
      "Batch Loss: 0.2988\n",
      "Predicted Labels: [2, 2, 4, 0, 4, 0, 4, 4, 4, 0, 3, 4, 4, 4, 4, 4]\n",
      "Ground Truth Labels: [2, 2, 4, 0, 4, 0, 4, 4, 4, 1, 3, 4, 4, 4, 4, 4]\n",
      "\n",
      "--- Batch 125/125 ---\n",
      "Batch Loss: 0.3992\n",
      "Predicted Labels: [4, 4, 4, 2, 4, 3, 4, 4, 2]\n",
      "Ground Truth Labels: [4, 4, 3, 2, 4, 3, 4, 4, 3]\n",
      "\n",
      "Evaluation abgeschlossen. Bereite finale Ergebnisse vor...\n",
      "\n",
      "Overall Accuracy: 0.7647\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Klasse 0       0.59      0.85      0.70       177\n",
      "    Klasse 1       0.34      0.23      0.27        97\n",
      "    Klasse 2       0.38      0.38      0.38       146\n",
      "    Klasse 3       0.55      0.27      0.36       271\n",
      "    Klasse 4       0.88      0.94      0.91      1302\n",
      "\n",
      "    accuracy                           0.76      1993\n",
      "   macro avg       0.55      0.53      0.52      1993\n",
      "weighted avg       0.74      0.76      0.74      1993\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T09:12:54.798027Z",
     "start_time": "2025-01-20T09:12:54.618106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_review(review_text):\n",
    "    # Tokenisiere den Eingabetext\n",
    "    encoding = tokenizer(\n",
    "        review_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Verschiebe die Tensoren zum Device (MPS, CUDA, CPU)\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    # Vorwärtsdurchlauf (Inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    return pred.item() + 1\n",
    "\n",
    "sample_text = \"the product was a piece of shit\"\n",
    "prediction = predict_review(sample_text)\n",
    "print(f\"Die vorhergesagte Sternebewertung: {prediction}\")\n"
   ],
   "id": "424fcd6d53de79cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die vorhergesagte Sternebewertung: 1\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
